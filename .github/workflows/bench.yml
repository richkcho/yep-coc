name: Manual PR Benchmarks

on:
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'Pull request number to benchmark'
        required: true
        type: number
      runner_label:
        description: 'Label for the self-hosted benchmark runner'
        required: true
        type: string

permissions:
  contents: read
  pull-requests: write

env:
  CARGO_TERM_COLOR: always
  CARGO_TARGET_DIR: ${{ github.workspace }}/.bench-target

jobs:
  benchmark:
    name: Bench PR #${{ inputs.pr_number }}
    runs-on: [self-hosted, '${{ inputs.runner_label }}']

    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@95d9a5deda9de15063e7595e9719c11c38c90ae2 # v2.13.2
        with:
          egress-policy: audit

      - name: Comment benchmark start on PR
        id: progress-comment
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            const prNumber = Number('${{ inputs.pr_number }}');
            const body = `Benchmark run started on runner label \`${{ inputs.runner_label }}\`...`;
            const { data: comment } = await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body,
            });
            core.setOutput('comment_id', comment.id);

      - name: Checkout PR head
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          ref: refs/pull/${{ inputs.pr_number }}/head
          fetch-depth: 0

      - name: Checkout main for baseline
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          ref: main
          path: main-baseline
          fetch-depth: 0

      - name: Record main commit
        id: main_sha
        run: echo "sha=$(git -C main-baseline rev-parse HEAD)" >> "$GITHUB_OUTPUT"

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@0b1efabc08b657293548b77fb76cc02d26091c7e # stable channel installer
        with:
          toolchain: stable

      - name: Cache Rust build artifacts
        uses: Swatinem/rust-cache@f13886b937689c021905a6b90929199931d60db1 # v2.8.1
        with:
          shared-key: bench-${{ inputs.runner_label }}
          workspaces: |
            .
            main-baseline
          cache-on-failure: true

      - name: Restore cached main baseline measurements
        id: baseline-cache
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4.3.0
        with:
          path: ${{ env.CARGO_TARGET_DIR }}/criterion
          key: criterion-main-${{ inputs.runner_label }}-${{ steps.main_sha.outputs.sha }}

      - name: Run baseline on main (only if missing)
        if: steps.baseline-cache.outputs.cache-hit != 'true'
        working-directory: main-baseline
        run: cargo bench --bench spsc-throughput -- --measurement-time 20 --sample-size 20 --save-baseline main

      - name: Save baseline cache (before PR run)
        if: steps.baseline-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830 # v4.3.0
        with:
          path: ${{ env.CARGO_TARGET_DIR }}/criterion
          key: criterion-main-${{ inputs.runner_label }}-${{ steps.main_sha.outputs.sha }}

      - name: Run benchmarks for PR against main baseline
        id: bench-pr
        run: |
          cargo bench --bench spsc-throughput -- --measurement-time 20 --sample-size 20 --baseline main 2>&1 | tee bench_output.txt

      - name: Upload criterion reports
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5.0.0
        with:
          name: pr-${{ inputs.pr_number }}-bench-${{ github.run_id }}
          path: ${{ env.CARGO_TARGET_DIR }}/criterion

      - name: Parse benchmark results
        id: parse-results
        run: |
          # Parse Criterion output to extract benchmark results
          # Look for lines with "change:" to identify performance deltas
          
          {
            echo "## Benchmark Results"
            echo ""
            echo "| Benchmark | Change | Status |"
            echo "|-----------|--------|--------|"
          } > results.md
          
          # Parse the benchmark output
          # Criterion outputs lines like:
          # "spsc/time_based/cap32_payload32_batch1 time:   [...]"
          # "                                        change: [+2.5% +5.0% +7.5%] (p = 0.00 < 0.05)"
          # "Performance has improved/regressed." or "No change in performance detected."
          
          awk '
            BEGIN { 
              # Default benchmark group prefix to strip from display names
              benchmark_group = "spsc/time_based/"
            }
            # Match benchmark name lines (pattern matches Criterion output format)
            $0 ~ "^[a-zA-Z_/]+ " && /time:/ {
              # Extract benchmark name (everything before "time:")
              match($0, /^[^ ]+/, arr)
              bench_name = arr[0]
              current_bench = bench_name
              current_status = ""
            }
            # Look for "change:" lines which indicate comparison results
            /change: \[/ {
              # Extract change percentage range
              match($0, /change: \[([^\]]+)\]/, arr)
              change = arr[1]
              current_change = change
            }
            # Capture status lines that follow change lines
            /Performance has regressed/ {
              current_status = "âš ï¸ Regressed"
            }
            /Performance has improved/ {
              current_status = "âœ… Improved"
            }
            /No change in performance detected/ {
              if (current_bench != "") {
                # Strip benchmark group prefix for cleaner display
                display_name = current_bench
                if (index(display_name, benchmark_group) == 1) {
                  display_name = substr(display_name, length(benchmark_group) + 1)
                }
                print "| `" display_name "` | - | âž– No Change |"
                current_bench = ""
              }
            }
            # When we see the next benchmark start, output the previous one if we have change data
            $0 ~ "^[a-zA-Z_/]+ " && /time:/ && current_bench != "" && current_change != "" {
              # Output previous benchmark before starting new one
              display_name = current_bench
              if (index(display_name, benchmark_group) == 1) {
                display_name = substr(display_name, length(benchmark_group) + 1)
              }
              status = current_status != "" ? current_status : "ðŸ“Š Changed"
              print "| `" display_name "` | " current_change " | " status " |"
              
              # Reset for new benchmark
              current_change = ""
              current_status = ""
              
              # Then process the new benchmark name
              match($0, /^[^ ]+/, arr)
              bench_name = arr[0]
              current_bench = bench_name
            }
            # Handle the last benchmark at end of file
            END {
              if (current_bench != "" && current_change != "") {
                display_name = current_bench
                if (index(display_name, benchmark_group) == 1) {
                  display_name = substr(display_name, length(benchmark_group) + 1)
                }
                status = current_status != "" ? current_status : "ðŸ“Š Changed"
                print "| `" display_name "` | " current_change " | " status " |"
              }
            }
          ' bench_output.txt >> results.md
          
          # If no results were found (only header lines remain), add a note
          # Count lines that look like benchmark results (containing backticks)
          result_count=$(tail -n +4 results.md | grep -c '`' || true)
          if [ "$result_count" -eq 0 ]; then
            echo "| No benchmark comparisons found | - | - |" >> results.md
          fi
          
          {
            echo ""
            echo "_Baseline: main @ \`${{ steps.main_sha.outputs.sha }}\`_"
          } >> results.md
          
          # Output for next step
          cat results.md

      - name: Comment benchmark artifact on PR
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            const fs = require('fs');
            const prNumber = Number('${{ inputs.pr_number }}');
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            const artifactName = `pr-${prNumber}-bench-${context.runId}`;
            
            // Read parsed results
            let resultsTable = '';
            try {
              resultsTable = fs.readFileSync('results.md', 'utf8');
            } catch (err) {
              resultsTable = '_No benchmark results could be parsed._';
            }
            
            const body = [
              `Benchmark run completed for PR #${prNumber}.`,
              ``,
              resultsTable,
              ``,
              `---`,
              ``,
              `- Run: ${runUrl}`,
              `- Artifact: ${artifactName} (reports + baseline/compare data)`
            ].join('\n');

            const commentId = "${{ steps.progress-comment.outputs.comment_id }}";
            if (commentId) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: commentId,
                body,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber,
                body,
              });
            }
